{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QueryExpanderTFIDF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "with open('final_sen_v2.txt', 'r') as f:\n",
        "  sen_v2 = f.read()\n",
        "\n",
        "sen_v2 = sen_v2.split('\\n')"
      ],
      "metadata": {
        "id": "b7Yc9SZax83n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TFIDF:\n",
        "  def __init__(self, a1):\n",
        "    self.docs = a1\n",
        "    self.vectorizer = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,1), analyzer='word')\n",
        "    self.vectorizer.fit_transform(self.docs)\n",
        "    self.stopwords = [x.strip() for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
        "  \n",
        "  def forward(self, querry, k=10, is_vector=False, words=[]):\n",
        "    ten_tokens = [0] * k\n",
        "    ten_scores = [-100] * k\n",
        "\n",
        "    if not is_vector:\n",
        "      vector = self.vectorizer.transform([querry])\n",
        "      b = vector.toarray()\n",
        "      q1 = querry.split()\n",
        "    else:\n",
        "      vector = querry\n",
        "      b = [vector.tolist()]\n",
        "      q1 = words\n",
        "      # print(words)\n",
        "      \n",
        "\n",
        "\n",
        "    NR = []\n",
        "\n",
        "    # print(len(b[0]))\n",
        "    \n",
        "    for item in tqdm(self.docs):\n",
        "      is_common = False\n",
        "      items = item.split()\n",
        "      for itemq in q1:\n",
        "        if itemq in items:\n",
        "          is_common = True\n",
        "      if not is_common:\n",
        "        NR.append(item)\n",
        "        continue\n",
        "      vector = self.vectorizer.transform([item])\n",
        "      c = vector.toarray()\n",
        "      if (norm(c[0])*norm(b[0])) != 0:\n",
        "        cos_sim = dot(c[0], b[0])/(norm(c[0])*norm(b[0]))\n",
        "        if cos_sim != 0:\n",
        "          # print(cos_sim)\n",
        "          # print(item)\n",
        "          for i in range(k):\n",
        "            if ten_scores[i] < cos_sim:\n",
        "              ten_scores[i] = cos_sim\n",
        "              ten_tokens[i] = item\n",
        "              break\n",
        "    # return ten_scores, ten_tokens\n",
        "    return ten_tokens, NR\n",
        "\n",
        "  def query_expansion(self, R, NR, query):\n",
        "    NR = random.sample(NR, 10)\n",
        "    R_vecs = []\n",
        "    NR_vecs = []\n",
        "    words = []\n",
        "    for item in R:\n",
        "      items = item.split()\n",
        "      for itemq in items:\n",
        "        if itemq not in self.stopwords:\n",
        "          words.append(itemq)\n",
        "      vector = self.vectorizer.transform([item])\n",
        "      c = np.array(vector.toarray()[0])\n",
        "      R_vecs.append(c)\n",
        "    words = random.sample(words, 6)\n",
        "    for oo in query.split():\n",
        "      words.append(oo)\n",
        "    R_vecs = np.array(R_vecs)\n",
        "    R_mean = np.mean(R_vecs, axis=0)\n",
        "    for item in NR:\n",
        "      vector = self.vectorizer.transform([item])\n",
        "      c = np.array(vector.toarray()[0])\n",
        "      NR_vecs.append(c)\n",
        "    NR_vecs = np.array(NR_vecs)\n",
        "    NR_mean = np.mean(NR_vecs, axis=0)\n",
        "    # out1, _ = self.forward(query, 30)\n",
        "    vector = R_vecs[-1]\n",
        "    # vector = self.vectorizer.transform(out1)\n",
        "    R_expanded = vector + 1.5 * R_mean - 0.1 * NR_mean\n",
        "    \n",
        "    en_tokens = 0\n",
        "    en_scores = -100\n",
        "\n",
        "    b = R_expanded\n",
        "    for i in range(len(R_vecs)):\n",
        "        c = R_vecs[i]\n",
        "        if (norm(c)*norm(b)) != 0:\n",
        "          cos_sim = dot(c, b)/(norm(c)*norm(b))\n",
        "          if cos_sim != 0:\n",
        "            if en_scores < cos_sim:\n",
        "              # print(1111111, en_scores, cos_sim)\n",
        "              en_scores = cos_sim\n",
        "              en_tokens = R\n",
        "              \n",
        "    return en_tokens"
      ],
      "metadata": {
        "id": "hh1TVy44yYdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from nltk import FreqDist\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import codecs\n",
        "# from hazm import *\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import codecs"
      ],
      "metadata": {
        "id": "rT-0gF8cyywg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b1 = TFIDF(sen_v2)\n",
        "q = 'کزین برتر'\n",
        "R, NR = b1.forward(q)\n",
        "expanded_query = b1.query_expansion(R, NR, q)[0]\n"
      ],
      "metadata": {
        "id": "99eAhHSwyu3x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}