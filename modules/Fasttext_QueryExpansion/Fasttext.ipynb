{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PpjQ6n0HtSWL"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm import tqdm\n",
        "from numpy import dot\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "jMmD2qhHarxt"
      },
      "outputs": [],
      "source": [
        "class FastText:\n",
        "  def __init__(self, word_vectors, a1 , fasttext_vectors,shahname_beyts, weight_type = 'simple', dims=100):\n",
        "    self.shahname_beyts = shahname_beyts\n",
        "    self.fasttext_vectors = fasttext_vectors\n",
        "    self.weight_type = weight_type\n",
        "    self.docs = a1\n",
        "    self.dims = dims\n",
        "    self.word_vectors = word_vectors\n",
        "    self.vectorizer = TfidfVectorizer(use_idf = True, norm ='l1', ngram_range=(1,1), analyzer='word')\n",
        "    self.vectorizer.fit_transform(self.docs)\n",
        "    self.strings = []\n",
        "    string = ''\n",
        "    for item in self.vectorizer.get_feature_names():\n",
        "      string += item\n",
        "      string += ' '\n",
        "      self.strings.append(item)\n",
        "    self.vector = self.vectorizer.transform([string])\n",
        "    self.vector = self.vector.toarray()\n",
        "    print('vectorizer sum = ', sum(self.vector))\n",
        "  \n",
        "\n",
        "    # print(self.vector)\n",
        "    # print(self.vector[0])\n",
        "  \n",
        "\n",
        "  \n",
        "  def get_fasttext_average_tf_idf_wights(self,mesra):\n",
        "    vector_sum = np.zeros(self.dims)\n",
        "    words = mesra.split(' ')\n",
        "    try:\n",
        "      for word in words:\n",
        "        index = self.strings.index(word)\n",
        "        vector_sum += self.word_vectors[word] * self.vector[0][index]\n",
        "    except Exception as e:\n",
        "      pass\n",
        "      # print(e.args[0])\n",
        "    return vector_sum\n",
        "  \n",
        "  def get_fastext_average(self,mesra):\n",
        "    counter = 0\n",
        "    vector_sum = np.zeros(self.dims)\n",
        "    words = mesra.split(' ')\n",
        "    for word in words:\n",
        "      try:\n",
        "        vector_sum += self.word_vectors[word]\n",
        "        counter += 1\n",
        "      except:\n",
        "        continue\n",
        "      vector_sum /= counter\n",
        "    return vector_sum\n",
        "\n",
        "  def get_fasttext(self, mesra):\n",
        "    if self.weight_type == 'tfidf':\n",
        "      return self.get_fasttext_average_tf_idf_wights(mesra)\n",
        "    elif self.weight_type == 'simple':\n",
        "      return self.get_fastext_average(mesra)\n",
        "\n",
        "  def forward(self,query):\n",
        "    vector = self.get_fasttext(query)\n",
        "    vector /= np.linalg.norm(vector)\n",
        "    final_vector = np.matmul(self.fasttext_vectors, np.transpose(vector))\n",
        "    sen_v2 = np.copy(self.docs)\n",
        "    R = np.array(final_vector).argsort()[-10:][::-1]\n",
        "    new_R =  np.unique(np.floor(R / 2)).astype(int)\n",
        "    print(R)\n",
        "    print(new_R)\n",
        "    ten_tokens = sen_v2[R]\n",
        "    ten_beyts = self.shahname_beyts[new_R]\n",
        "    ten_scores = final_vector[R]\n",
        "    vocab_error = 0\n",
        "    # ten_tokens = [0] * k\n",
        "    # ten_scores = [-100] * k\n",
        "    # vocab_error = 0\n",
        "    # b  = self.get_fasttext(query)\n",
        "\n",
        "    # for item in tqdm(self.docs):\n",
        "    #   try:\n",
        "    #     c  = self.get_fasttext(item)\n",
        "    #     if (norm(c)*norm(b)) != 0:\n",
        "    #       cos_sim = dot(c, b)/(norm(c)*norm(b))\n",
        "    #       if cos_sim != 0:\n",
        "    #         for i in range(k):\n",
        "    #           if ten_scores[i] < cos_sim:\n",
        "    #             ten_scores[i] = cos_sim\n",
        "    #             ten_tokens[i] = item\n",
        "    #             break\n",
        "    #   except Exception as e:\n",
        "    #     vocab_error += 1\n",
        "    #     continue\n",
        "\n",
        "    # for i in range(len(ten_scores)):\n",
        "    #   for j in range(len(ten_scores)):\n",
        "    #     if ten_scores[i] > ten_scores[j]:\n",
        "    #        ten_scores[i], ten_scores[j] = ten_scores[j], ten_scores[i]\n",
        "    #        ten_tokens[i], ten_tokens[j] = ten_tokens[j], ten_tokens[i]\n",
        "\n",
        "    return vocab_error, ten_tokens, ten_scores , ten_beyts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_real_shahname_beyts():\n",
        "  try:\n",
        "      # Fix UTF8 output issues on Windows console.\n",
        "      # Does nothing if package is not installed\n",
        "      from win_unicode_console import enable\n",
        "      enable()\n",
        "  except ImportError:\n",
        "      pass\n",
        "  # my_file = open(\"rahavard_365.txt\", \"r\")\n",
        "  my_file = open(\"shahname_data.txt\", \"r\")\n",
        "    \n",
        "  # reading the file\n",
        "  data = my_file.read()\n",
        "    \n",
        "  # replacing end of line('/n') with ' ' and\n",
        "  # splitting the text it further when '.' is seen.\n",
        "  lst = data.split(\"\\n\")\n",
        "\n",
        "  # printing the data\n",
        "  my_file.close()\n",
        "  lst2 = []\n",
        "  for item in lst:\n",
        "    x = item.split('****')\n",
        "    lst2.append(x)\n",
        "  # beyts = []\n",
        "  beyts_ = []\n",
        "  for item in lst2:\n",
        "    if len(item) == 2:\n",
        "      beyts_.append(item[0] + '****' + item[1])\n",
        "  return np.array(beyts_)"
      ],
      "metadata": {
        "id": "Tf619pIY_I23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "5KaNG0WxTCYF"
      },
      "outputs": [],
      "source": [
        "# run only once\n",
        "k = 10 # number of similar beyts to return\n",
        "word_vectors = KeyedVectors.load_word2vec_format('drive/MyDrive/vec_file.vec')\n",
        "with open('drive/MyDrive/sen_v2.txt', 'r') as f:\n",
        "  sen_v2 = f.read()\n",
        "sen_v2 = sen_v2.split('\\n')\n",
        "sen_v2 = np.array(sen_v2)\n",
        "fasttext_vectors = np.loadtxt('drive/MyDrive/fasttext_vectors.txt')\n",
        "nan_indexes = []\n",
        "for i in range(len(fasttext_vectors)):\n",
        "  if(np.isnan(fasttext_vectors[i]).any()):\n",
        "    nan_indexes.append(i)\n",
        "    if i % 2 == 0:\n",
        "      nan_indexes.append(i + 1)\n",
        "    else:\n",
        "      nan_indexes.append(i - 1)\n",
        "nan_beyt_indexes = np.unique(np.floor(np.array(nan_indexes)/ 2)).astype(int)\n",
        "shahname_beyts = get_real_shahname_beyts()\n",
        "shahname_beyts = np.delete(shahname_beyts, nan_beyt_indexes)\n",
        "fasttext_vectors= np.delete(fasttext_vectors, nan_indexes, axis = 0)\n",
        "\n",
        "sen_v2= np.delete(sen_v2, nan_indexes, axis = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext1_2 = FastText(word_vectors, sen_v2,  fasttext_vectors,shahname_beyts, 'simple' )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QK70Nt4BVbM",
        "outputId": "4dc1233a-0a63-4b38-8500-be8e0f89fcc9"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectorizer sum =  [5.94364102e-05 2.98065708e-05 3.71215122e-05 ... 6.15497739e-05\n",
            " 5.17935736e-05 6.15497739e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "sc3tkDHZ4SIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8bbed5-f4ec-420d-f2fb-30d97fe6110d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  104 26964 27747 40986 86978 72576 91074 22404 55371 69098]\n",
            "[   52 11202 13482 13873 20493 27685 34549 36288 43489 45537]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ببالد ندارد جز این نیرویی****نپوید چو پیوندگان هر سویی',\n",
              "       'من این دانم ار هست پاسخ جزین****فراخست رای جهان آفرین',\n",
              "       'سگالش چنین بد نوشته جزین****نکرد ایچ یاد از جهان آفرین',\n",
              "       'یکی اسب دیدم نگونسار زین****ز بیژن نشانی ندارد جزین',\n",
              "       'تو این اندکی لشکر من مبین****مراجوی با گرز بر پشت زین',\n",
              "       'چو کژ آفریدش جهان آفرین****تو مشنو سخن زو و کژی مبین',\n",
              "       'چو دیدی بگویش کزین سوگرای****ز نزدیک ماکن سوی خانه رای',\n",
              "       'ندانم که دیدار باشد جزین****یک امشب بکوشیم دست پسین',\n",
              "       'ندانم که دیدار باشد جزین****که داند چنین جز جهان آفرین',\n",
              "       'بر و یال و کتف سیاوش جزین****نخواهد کمان نیز بر دشت کین'],\n",
              "      dtype='<U67')"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "# fasttext_query\n",
        "words_not_found, mesra, scores, beyts = fasttext1_2.forward('کزین برتر')\n",
        "beyts"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Fasttext.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}